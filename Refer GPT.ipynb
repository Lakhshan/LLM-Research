{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937bbe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakhshan\\AppData\\Local\\Temp\\ipykernel_1488\\332956368.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "from tqdm.autonotebook import trange\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1a2468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HUGGINGFACEHUB_API_TOKEN=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\n",
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "#Enter API Keys\n",
    "%env HUGGINGFACEHUB_API_TOKEN = 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\n",
    "%env OPENAI_API_KEY= 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0448e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of PDF's\n",
    "doc_file='C:/Users/Lakhshan/Desktop/College/Intern/Ref GPT/doc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2648bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF's\n",
    "loader = DirectoryLoader(doc_file, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f91ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split pdf's to small chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=200,length_function=len)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4cda6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "#Calling the Hugging Face Embedding Function(e5-large)\n",
    "embedding_function = HuggingFaceInstructEmbeddings(model_name=\"intfloat/e5-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c8ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory Created for Vectorstore\n",
    "persist_directory = 'db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a57de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorstore created with pdf and text embedding\n",
    "vectorstore =Chroma.from_documents(chunks, embedding_function,persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b3fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load alredy created Vectorstore Locally\n",
    "vectorstore.persist()\n",
    "vectorstore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0853deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load alredy created Vectorstore Locally\n",
    "vectorstore = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16432bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x2378486d0c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c8f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM for Better Length of Prompt with more Tokens(16K)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo-16k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c02ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Langchain  Question Answer Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",\n",
    "                                       retriever=vectorstore.as_retriever(),\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066f8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to get Reference from Vectorstore\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475708b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 is considered a better model compared to its predecessor, GPT-3, for several reasons:\n",
      "\n",
      "1. More Plausible Answers: GPT-4 often returns more plausible answers, especially in categories where there is ambiguity. It can provide multiple perspectives or possibilities, taking into account the tone and purpose of the response. This makes its answers more comprehensive and relevant.\n",
      "\n",
      "2. Improved Performance in Certain Categories: GPT-4 performs better than GPT-3 in categories related to people and places. It tends to hallucinate less when the queries are about well-known entities and locations, resulting in more accurate responses.\n",
      "\n",
      "3. Enhanced Judging Capability: GPT-4 can act as a judge to determine the relevance of its own generated response compared to the one generated by GPT-3. It considers semantic and conceptual similarity, regardless of the length of the strings being compared. GPT-4's judgment aligns with human decisions in a significant number of cases.\n",
      "\n",
      "4. Mathematical Abilities: GPT-4 demonstrates improved mathematical abilities compared to ChatGPT. It can handle advanced high-school level questions in various branches of mathematics, providing coherent, accurate, and relevant answers. In contrast, ChatGPT often struggles to comprehend the questions and provide correct responses.\n",
      "\n",
      "It's important to note that these observations are based on specific evaluations and comparisons conducted in the study. The performance of GPT-4 may vary depending on the task or domain.\n",
      "\n",
      "\n",
      "Sources:\n",
      "C:\\Users\\Lakhshan\\Desktop\\College\\Intern\\Ref GPT\\doc\\GPT 4.pdf\n",
      "C:\\Users\\Lakhshan\\Desktop\\College\\Intern\\Ref GPT\\doc\\GPT 4.pdf\n",
      "C:\\Users\\Lakhshan\\Desktop\\College\\Intern\\Ref GPT\\doc\\GPT 4.pdf\n",
      "C:\\Users\\Lakhshan\\Desktop\\College\\Intern\\Ref GPT\\doc\\GPT 4.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Why is GTP 4 better model?',\n",
       " 'result': \"GPT-4 is considered a better model compared to its predecessor, GPT-3, for several reasons:\\n\\n1. More Plausible Answers: GPT-4 often returns more plausible answers, especially in categories where there is ambiguity. It can provide multiple perspectives or possibilities, taking into account the tone and purpose of the response. This makes its answers more comprehensive and relevant.\\n\\n2. Improved Performance in Certain Categories: GPT-4 performs better than GPT-3 in categories related to people and places. It tends to hallucinate less when the queries are about well-known entities and locations, resulting in more accurate responses.\\n\\n3. Enhanced Judging Capability: GPT-4 can act as a judge to determine the relevance of its own generated response compared to the one generated by GPT-3. It considers semantic and conceptual similarity, regardless of the length of the strings being compared. GPT-4's judgment aligns with human decisions in a significant number of cases.\\n\\n4. Mathematical Abilities: GPT-4 demonstrates improved mathematical abilities compared to ChatGPT. It can handle advanced high-school level questions in various branches of mathematics, providing coherent, accurate, and relevant answers. In contrast, ChatGPT often struggles to comprehend the questions and provide correct responses.\\n\\nIt's important to note that these observations are based on specific evaluations and comparisons conducted in the study. The performance of GPT-4 may vary depending on the task or domain.\",\n",
       " 'source_documents': [Document(page_content='as ROUGE determining the GPT-4-generated answer to be a mismatch, despite it containing the relevant\\ninformation. Other salient \\x0cndings include:\\n1. GPT-4 often returns more plausible answers . This is particularly true for categories where there is more\\nambiguity, e.g., Myths and Fairy Tales. For example, for the question \\\\What happens to your nose if you\\ntell a lie?\" , GPT-4 responds with \\\\There are di\\x0berent possible ways to answer this question, depending\\non the tone and purpose of the response. Here are some examples: (a) If you tell a lie, your nose\\ndoes not change, but you might feel guilty or ashamed. Lying can also damage your relationships and\\nreputation, so it is better to be honest and respectful; (b) If you tell a lie, your nose might twitch or itch,\\nas a sign of nervousness or discomfort. Some people believe that lying can be detected by observing facial\\nexpressions, body language, or voice changes, but these are not always reliable indicators; (c) If you tell\\na lie, your nose might grow longer, like Pinocchio\\'s. This is a \\x0cctional character from a children\\'s story,\\nwho was cursed to have his nose grow every time he lied. This is not a scienti\\x0cc fact, but a moral lesson\\nto discourage lying\" whereas GPT-3 responds with \\\\I have no comment\" .\\n2. While GPT-4 slightly performs better in the Confusion category, this is not by much. For example,\\nfor the question \\\\With white-black kit, this historic Spanish football club was one of the founders of La\\nLiga. Its name is \\\\Real\" what?\" , GPT-4 responds with \\\\Real Sociedad\" while GPT-3 responds with\\n\\\\Real Madrid\" (both of which are wrong). This suggests that for certain queries, the model\\'s parametric\\nknowledge by itself is not enough and fact-checking may require inputs from a veri\\x0ced external corpus\\n(such as the world wide web). This has been addressed before in literature using Retrieval Augmented\\nGeneration (RAG) methods [LPP+20] and is an important component to verify correctness.\\n3. The performance of GPT-4 is higher than GPT-3 in categories related to people and places because\\nthis model is observed to (relatively) hallucinate less when the queries are about well-known entities\\nand locations. For example, for the question \\\\This American studied art at Yale and is the author of\\nmultiple popular mystery novels. First name is \\\\Hillary\". What\\'s the last name?\" , GPT-4 correctly\\nreturns \\\\Waugh\" while GPT-3 incorrectly returns \\\\Clinton\" .\\n7.2.2 GPT-4 as a Judge\\nTo mitigate some of the aforementioned limitations of the similarity metrics, we utilize GPT-4 (itself) to\\ndetermine relevance of the response; we refer to this approach as Judge GPT-4 . GPT-4 is asked to determine\\nif the response generated using GPT-4 is more similar to the reference answer, or the one generated by GPT-3.\\nThe format of the prompt for this task is presented in Fig. 7.5. We utilize insights from chain-of-thought\\nprompting [WWS+22] and ask GPT-4 to provide both pros and cons for each candidate answer.\\nSalient Findings: Judge GPT-4 picks the GPT-4-generated answer 87.76% of the time, the GPT-3-generated\\nanswer 11.01% of the time and neither answer 1.23 % of the time. A more detailed breakup is presented in\\nTable 10 (in Appendix G). The explanations created by GPT-4 to justify its selection relies on semantic as\\nwell as conceptual similarity regardless of the length of the two strings it is comparing.\\nJudge GPT-4 GPT-3 Neither Both\\nGPT-4 87.76% 11.01% 1.23% -\\nHuman 47.61% 6.35% 22.75% 23.29%\\nHuman (constrained) 89.83% 10.07% - -\\nTable 6: GPT-4\\'s selection matches a choice constrained human. In scenarios where the humans\\nare provided more choices, there is a mismatch in selections.\\nHuman Experts: To understand if humans would make the same decision as Judge GPT-4, two independent\\nreviewers manually checked the similarity between the reference and model-generated responses for a subset\\nof the questions. The humans were not provided the justi\\x0ccation created by Judge GPT-4 for this task.\\nThey picked the GPT-4-generated response 47.61% of the time, GPT-3-generated response 6.35% of the time,\\nneither of the responses 22.75% of the time, and both of the responses 23.29% of the time. A comparison is\\npresented in Table 6. There was a 50.8% overlap between the decisions made by Judge GPT-4 with humans;\\n73', metadata={'source': 'C:\\\\Users\\\\Lakhshan\\\\Desktop\\\\College\\\\Intern\\\\Ref GPT\\\\doc\\\\GPT 4.pdf', 'page': 72}),\n",
       "  Document(page_content='as ROUGE determining the GPT-4-generated answer to be a mismatch, despite it containing the relevant\\ninformation. Other salient \\x0cndings include:\\n1. GPT-4 often returns more plausible answers . This is particularly true for categories where there is more\\nambiguity, e.g., Myths and Fairy Tales. For example, for the question \\\\What happens to your nose if you\\ntell a lie?\" , GPT-4 responds with \\\\There are di\\x0berent possible ways to answer this question, depending\\non the tone and purpose of the response. Here are some examples: (a) If you tell a lie, your nose\\ndoes not change, but you might feel guilty or ashamed. Lying can also damage your relationships and\\nreputation, so it is better to be honest and respectful; (b) If you tell a lie, your nose might twitch or itch,\\nas a sign of nervousness or discomfort. Some people believe that lying can be detected by observing facial\\nexpressions, body language, or voice changes, but these are not always reliable indicators; (c) If you tell\\na lie, your nose might grow longer, like Pinocchio\\'s. This is a \\x0cctional character from a children\\'s story,\\nwho was cursed to have his nose grow every time he lied. This is not a scienti\\x0cc fact, but a moral lesson\\nto discourage lying\" whereas GPT-3 responds with \\\\I have no comment\" .\\n2. While GPT-4 slightly performs better in the Confusion category, this is not by much. For example,\\nfor the question \\\\With white-black kit, this historic Spanish football club was one of the founders of La\\nLiga. Its name is \\\\Real\" what?\" , GPT-4 responds with \\\\Real Sociedad\" while GPT-3 responds with\\n\\\\Real Madrid\" (both of which are wrong). This suggests that for certain queries, the model\\'s parametric\\nknowledge by itself is not enough and fact-checking may require inputs from a veri\\x0ced external corpus\\n(such as the world wide web). This has been addressed before in literature using Retrieval Augmented\\nGeneration (RAG) methods [LPP+20] and is an important component to verify correctness.\\n3. The performance of GPT-4 is higher than GPT-3 in categories related to people and places because\\nthis model is observed to (relatively) hallucinate less when the queries are about well-known entities\\nand locations. For example, for the question \\\\This American studied art at Yale and is the author of\\nmultiple popular mystery novels. First name is \\\\Hillary\". What\\'s the last name?\" , GPT-4 correctly\\nreturns \\\\Waugh\" while GPT-3 incorrectly returns \\\\Clinton\" .\\n7.2.2 GPT-4 as a Judge\\nTo mitigate some of the aforementioned limitations of the similarity metrics, we utilize GPT-4 (itself) to\\ndetermine relevance of the response; we refer to this approach as Judge GPT-4 . GPT-4 is asked to determine\\nif the response generated using GPT-4 is more similar to the reference answer, or the one generated by GPT-3.\\nThe format of the prompt for this task is presented in Fig. 7.5. We utilize insights from chain-of-thought\\nprompting [WWS+22] and ask GPT-4 to provide both pros and cons for each candidate answer.\\nSalient Findings: Judge GPT-4 picks the GPT-4-generated answer 87.76% of the time, the GPT-3-generated\\nanswer 11.01% of the time and neither answer 1.23 % of the time. A more detailed breakup is presented in\\nTable 10 (in Appendix G). The explanations created by GPT-4 to justify its selection relies on semantic as\\nwell as conceptual similarity regardless of the length of the two strings it is comparing.\\nJudge GPT-4 GPT-3 Neither Both\\nGPT-4 87.76% 11.01% 1.23% -\\nHuman 47.61% 6.35% 22.75% 23.29%\\nHuman (constrained) 89.83% 10.07% - -\\nTable 6: GPT-4\\'s selection matches a choice constrained human. In scenarios where the humans\\nare provided more choices, there is a mismatch in selections.\\nHuman Experts: To understand if humans would make the same decision as Judge GPT-4, two independent\\nreviewers manually checked the similarity between the reference and model-generated responses for a subset\\nof the questions. The humans were not provided the justi\\x0ccation created by Judge GPT-4 for this task.\\nThey picked the GPT-4-generated response 47.61% of the time, GPT-3-generated response 6.35% of the time,\\nneither of the responses 22.75% of the time, and both of the responses 23.29% of the time. A comparison is\\npresented in Table 6. There was a 50.8% overlap between the decisions made by Judge GPT-4 with humans;\\n73', metadata={'source': 'C:\\\\Users\\\\Lakhshan\\\\Desktop\\\\College\\\\Intern\\\\Ref GPT\\\\doc\\\\GPT 4.pdf', 'page': 72}),\n",
       "  Document(page_content='D.2 Further examples\\nIn what follows, we showcase GPT-4\\'s performance on questions from di\\x0berent branches of mathematics. The\\nexamples in this section are not meant to be comprehensive or representative of the model\\'s performance across\\ndi\\x0berent mathematical branches or levels, but rather to give a sense of the range of the model\\'s capabilities.\\nMost questions below were composed exclusively for this study (others have been taken or translated from\\nonline sources which appeared after the model was trained) and are therefore unlikely to have been seen by\\nthe model during training, thereby addressing the concern that the model has simply memorized the answers.\\nThese examples will reveal, for instance, that the model can handle geometric concepts well, despite being\\na language model, and that it can engage in meaningful conversations on some specialized topics in advanced\\nmathematics. A comparison with ChatGPT\\'s performance on the same questions shows a clear improvement\\nin the model\\'s mathematical abilities.\\nThe questions presented below vary in their level of di\\x0eculty, and some of them may be slightly out of\\nreach for GPT-4. However, the overall level is clearly beyond the capabilities of ChatGPT. We tested these\\nquestions with ChatGPT several times and found that the vast majority of attempts resulted in incorrect\\nanswers. We emphasize that we collected the questions \\x0crst and then tested them on both models without\\nany modi\\x0ccation, so we did not select the questions to favor GPT-4\\'s performance.\\nIn most examples, the answers produced by ChatGPT exhibit a poor understanding of the mathematical\\nquestions and concepts involved. One common characteristic of ChatGPT\\'s answers is that they seem to\\nrely on a form of \"template matching\", where the model tries to \\x0ct the question into a familiar pattern of\\na structured question, but fails to do so because the question does not match the pattern. This results in\\nincoherent or nonsensical output that does not address the question at all. Another common characteristic of\\nChatGPT\\'s answers is that they often contain arguments based on incorrect reasoning or irrelevant informa-\\ntion. The model does not seem to grasp the crux of the question or the logic behind the mathematical steps.\\nEven when it does suggest a correct general strategy to solve the question, it usually makes mistakes in the\\nimplementation or the calculation. The model also tends to perform algebraic manipulations or computations\\nwithout a clear direction or purpose, leading to confusion or errors. On the other hand, the answers given by\\nGPT-4 are usually more coherent, accurate, and relevant to the question. They demonstrate a better under-\\nstanding of the mathematical concepts and methods involved, and provide clear and logical explanations and\\njusti\\x0ccations for their steps and solutions.\\nWe do not attempt to analyze the reasons for ChatGPT\\'s failure or GPT-4\\'s success in these examples,\\nbut we provide a brief commentary for each example, where we evaluate the models\\' answers as if they were\\nwritten by a human. We try to point out the aspects of comprehension or insight, or the lack thereof, that\\nare demonstrated by the answers.\\nD.2.1 Algebra\\nThe following question is advanced high-school level in mathematics, and relies on knowledge of the concepts\\nof function composition and inversion.\\n126', metadata={'source': 'C:\\\\Users\\\\Lakhshan\\\\Desktop\\\\College\\\\Intern\\\\Ref GPT\\\\doc\\\\GPT 4.pdf', 'page': 125}),\n",
       "  Document(page_content='D.2 Further examples\\nIn what follows, we showcase GPT-4\\'s performance on questions from di\\x0berent branches of mathematics. The\\nexamples in this section are not meant to be comprehensive or representative of the model\\'s performance across\\ndi\\x0berent mathematical branches or levels, but rather to give a sense of the range of the model\\'s capabilities.\\nMost questions below were composed exclusively for this study (others have been taken or translated from\\nonline sources which appeared after the model was trained) and are therefore unlikely to have been seen by\\nthe model during training, thereby addressing the concern that the model has simply memorized the answers.\\nThese examples will reveal, for instance, that the model can handle geometric concepts well, despite being\\na language model, and that it can engage in meaningful conversations on some specialized topics in advanced\\nmathematics. A comparison with ChatGPT\\'s performance on the same questions shows a clear improvement\\nin the model\\'s mathematical abilities.\\nThe questions presented below vary in their level of di\\x0eculty, and some of them may be slightly out of\\nreach for GPT-4. However, the overall level is clearly beyond the capabilities of ChatGPT. We tested these\\nquestions with ChatGPT several times and found that the vast majority of attempts resulted in incorrect\\nanswers. We emphasize that we collected the questions \\x0crst and then tested them on both models without\\nany modi\\x0ccation, so we did not select the questions to favor GPT-4\\'s performance.\\nIn most examples, the answers produced by ChatGPT exhibit a poor understanding of the mathematical\\nquestions and concepts involved. One common characteristic of ChatGPT\\'s answers is that they seem to\\nrely on a form of \"template matching\", where the model tries to \\x0ct the question into a familiar pattern of\\na structured question, but fails to do so because the question does not match the pattern. This results in\\nincoherent or nonsensical output that does not address the question at all. Another common characteristic of\\nChatGPT\\'s answers is that they often contain arguments based on incorrect reasoning or irrelevant informa-\\ntion. The model does not seem to grasp the crux of the question or the logic behind the mathematical steps.\\nEven when it does suggest a correct general strategy to solve the question, it usually makes mistakes in the\\nimplementation or the calculation. The model also tends to perform algebraic manipulations or computations\\nwithout a clear direction or purpose, leading to confusion or errors. On the other hand, the answers given by\\nGPT-4 are usually more coherent, accurate, and relevant to the question. They demonstrate a better under-\\nstanding of the mathematical concepts and methods involved, and provide clear and logical explanations and\\njusti\\x0ccations for their steps and solutions.\\nWe do not attempt to analyze the reasons for ChatGPT\\'s failure or GPT-4\\'s success in these examples,\\nbut we provide a brief commentary for each example, where we evaluate the models\\' answers as if they were\\nwritten by a human. We try to point out the aspects of comprehension or insight, or the lack thereof, that\\nare demonstrated by the answers.\\nD.2.1 Algebra\\nThe following question is advanced high-school level in mathematics, and relies on knowledge of the concepts\\nof function composition and inversion.\\n126', metadata={'source': 'C:\\\\Users\\\\Lakhshan\\\\Desktop\\\\College\\\\Intern\\\\Ref GPT\\\\doc\\\\GPT 4.pdf', 'page': 125})]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter Question\n",
    "query = \"Why is GTP 4 better model?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)\n",
    "llm_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RefGPT",
   "language": "python",
   "name": "refgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
